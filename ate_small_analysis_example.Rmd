---
title: "Small example of ATE estimation with missing attributes"
author: "Imke Mayer"
output: 
  html_document:
    fig_caption: yes
    number_sections: yes
    toc: yes
    toc_depth: 4
    toc_float: yes
references:
- id: mayer2020
  title: Doubly robust treatment effect estimation with missing attributes
  author:
  - family: Mayer
    given: Imke
  - family: Sverdrup
    given: Erik
  - family: Gauss
    given: Tobias
  - family: Moyer
    given: Jean-Denis
  - family: Wager
    given: Stefan
  - family: Josse
    given: Julie
  container-title: Annals of Applied Statistics
  volume: 14
  URL: 'http://dx.doi.org/10.1214/20-AOAS1356'
  DOI: 10.1214/20-AOAS1356
  issue: 3
  page: 1409-1431
  type: article-journal
  issued:
    year: 2020
- id: rmisstastic
  title: 'R-miss-tastic: a unified platform for missing values methods and workflows'
  author:
  - family: Mayer
    given: Imke
  - family: Josse
    given: Julie
  - family: Tierney
    given: Nicholas
  - family: Vialaneix
    given: Nathalie
  archivePrefix: arXiv
  eprint: 1908.04822
  container-title: arXiv preprint
  URL: 'https://arxiv.org/abs/1908.04822'
  issued:
    year: 2019
- id: le2008
  title: 'FactoMineR: An R package for multivariate analysis'
  author:
  - family: Lê
    given: Sébastien
  - family: Josse
    given: Julie
  - family: Husson
    given: François
  container-title: Journal of statistical software
  volume: 25
  URL: 'https://doi.org/10.18637/jss.v025.i01'
  DOI: 10.18637/jss.v025.i01
  issue: 1
  page: 1-18
  type: article-journal
  publisher: Foundation for Open Access Statistics
  issued:
    year: 2008
- id: seaman2014
  title: Inverse Probability Weighting with Missing Predictors of Treatment Assignment or Missingness
  author:
  - family: Seaman
    given: Shaun
  - family: White
    given: Ian
  container-title: Communications in Statistics - Theory and Methods
  volume: 43
  URL: 'https://doi.org/10.1080/03610926.2012.700371'
  DOI: 10.1080/03610926.2012.700371
  issue: 16
  page: 3499-3515
  type: article-journal
  publisher: Taylor & Francis
  issued:
    year: 2014
- id: athey2019
  title: Generalized random forests
  author:
  - family: Athey
    given: Susan
  - family: Tibshirani
    given: Julie
  - family: Wager
    given: Stefan
  container-title: Annals of Statistics
  volume: 47
  URL: 'https://doi.org/10.1214/18-AOS1709'
  DOI: 10.1214/18-AOS1709
  issue: 2
  page: 1148-1178
  type: article-journal
  issued:
    year: 2019
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache= TRUE)
```

# Introduction

The objective of this notebook is to illustrate how to perform treatment effect 
estimation with missing attributes on a small toy example.
By replacing the toy example with the dataset of your choice (following the format
instructions below) you can estimate average treatment effects (ATE) on your own
data.

For more information and details on the theoretical background of the methodology,
we refer to [[@mayer2020]](http://dx.doi.org/10.1214/20-AOAS1356).

**You can replace the chunks `rename_data` and `confounders` with any data set you wish to analyze. 
For this, you have to specify:**

- `X.na`: confounders. A data.frame of size `#observations x #covariates`. With or without missing values.
- `W`: treatment assignment. A binary vector coded either with `{0,1}` or with `{FALSE,TRUE}` (representing `{control,treatment}`). Without missing values.
- `Y`: observed outcome. A numerical or binary vector (if binary, then coded with `{0,1}`). Without missing values.

And at the end of the chunk `rename_data`, set `synthetic=FALSE`, if you choose your
own data set.

If `X.na` contains confounders but also other covariates that are not to be 
considered as confounders in the analysis, specify the names of the confounders
in the chunk `confounders` and the role of the remaining covariates 
(either treatment regressors or outcome regressors).

# Preliminaries

## Load libraries

```{r load_libraries, results='hide', message=F, warning=F}
library(cobalt)
library(ggplot2)
library(dplyr)
library(MASS)
library(pracma)
library(assertthat)

library(caret) # 
library(ranger) # Random Forests prediction
library(FactoMineR) # Factorial data analysis
library(grf) # Doubly robust treatment effect estimation

library(norm)
library(missMDA) # PCA/MCA with missing values + iterative PC imputation
library(mice) # Multiple imputation 
library(VIM) # Missing values exploration and visualization

library(devtools)
# Load the ATE estimation functions `ipw` and `dr` from the GitHub repository
source_url("https://raw.githubusercontent.com/imkemayer/causal-inference-missing/master/Helper/helper_causalInference.R")
# Load function to generate missing values using different mechanisms from R-miss-tastic
source_url("https://rmisstastic.netlify.app/how-to/generate/amputation.R")

# Set random generator seed for reproducible results
set.seed(1234)
```

```{r ggplot_details, echo=F, warning=F, message=F}
library(RColorBrewer) # Color scale
myColors <- brewer.pal(6,"Set1")
names(myColors) <- c("-", "pc.imp", "mf", "mia", "mice", "mean")
myColorsATE <- c("black", "cyan")
names(myColorsATE) <- c("ATE true", "ATE naive")
colScale <- scale_colour_manual(name = c("", ""), values = c(myColors, myColorsATE))
```


## Generate toy example

We will generate a simple toy data set with normally distributed confounders and
MCAR missing values. For generating missing values, we use code provided by 
[[@rmisstastic]](https://arxiv.org/abs/1908.04822).

```{r choose_parameters}
seed <- 4321
n <- 5000
p <- 5
tau <- 1
```

```{r generate_toy_data}
rho <- 0.3
Sigma <- diag(p) + rho*upper.tri(diag(p)) + rho*lower.tri(diag(p))
X <- mvrnorm(n=n, mu=rep(1, p), Sigma=Sigma)
res.na <- produce_NA(X, mechanism="MCAR", perc.missing=0.3, seed=seed)
X.na <- res.na$data.incomp

# nonlinear transformations of X
X.tmp <- X
for (j in 1:p){
  X.tmp[,j] <- (mod(j,5)==1)*((X.tmp[,j]<quantile(X.tmp[,j],0.7)) + (X.tmp[,j]> quantile(X.tmp[,j],0.2))) +
               (mod(j,5)==2)*(1/(0.001+exp(X.tmp[,j]*X.tmp[,1]))) +
               (mod(j,5)==3)*(-(X.tmp[,j])*(X.tmp[,2]>0)) +
               (mod(j,5)==4)*(-2.5*sqrt(abs(X.tmp[,j]))) +
               (mod(j,5)==0)*(X.tmp[,3]*X.tmp[,j])
} 

# propensity scores
X.tmp.ps <- X.tmp
X.tmp.ps[res.na$idx_newNA] <- 0
expit <- function(x){ return(1/(1+exp(-x))) }
alpha <- array(c(-0.6, 0.6), dim=p)
prop_scores <- apply(data.frame(X.tmp), MARGIN=1, 
                          FUN=function(z) expit(z%*%alpha))
prop_scores <- 0.01 + (pmin(0.98,prop_scores) - min(prop_scores))/(max(prop_scores)-min(prop_scores))
W <- sapply(prop_scores, FUN=function(p) rbinom(n=1, size=1, prob=p))
  
 
# outcome 
epsilons <- rnorm(n, sd=0.2)
Y <- rep(0,n)
beta <- runif(p, -1, 1)

Y[which(W==1)] <- sign(X.tmp[which(W==1),]%*%beta)*log(abs(X.tmp[which(W==1),]%*%beta)) + tau + epsilons[which(W==1)]
Y[which(!(W==1))] <- sign(X.tmp[which(!(W==1)),]%*%beta)*log(abs(X.tmp[which(!(W==1)),]%*%beta)) + epsilons[which(!(W==1))]

toy_data <- list(X=X, X.na=X.na, W=W, Y=Y)
```


**If you want to use your own data set, change the following chunk, following the
instructions given in the introduction.**
```{r rename_data}
synthetic <- TRUE

X.na <- data.frame(toy_data$X.na)
W <- toy_data$W
Y <- toy_data$Y

covariate_names <- colnames(X.na)
n <- dim(X.na)[1]
p <- dim(X.na)[2]

df.na <- data.frame(cbind(X.na, W=W, Y=Y))
colnames(df.na) <- c(covariate_names, "W", "Y")
```



In certain cases, not all covariates are necessarily confounders. Therefore we 
specify the set of confounders (here we consider all covariates to be confounders).
Additionally, in some cases there are also variables that are only predictive of 
the treatment assignment. Their names can be specified in `only_treatment_pred_names`.
```{r, confounders}
confounder_names <- covariate_names
only_treatment_pred_names <- c()
only_outcome_pred_names <- setdiff(covariate_names, 
                                   c(confounder_names, only_treatment_pred_names))
```


First we compute the unadjusted ATE, ignoring the confounders.

```{r unadjusted_ate}
ate_raw <- mean(Y[which(as.logical(W))]) - mean(Y[which(!as.logical(W))])
```

This gives us $ATE_{naive}=$ `r ate_raw`.

Let us also assess how the outcome is distributed with respect to the treatment 
group.

```{r conditional_distribution, echo=F} 
if (length(unique(Y))==2){
  res.table <- table(as.factor(W), as.factor(Y), 
                   dnn = c("W==1","Y==1"))
  prop.table(res.table)
} else {
  ggplot(data.frame(W=factor(W, labels=c("Control", "Treatment")), Y=Y)) + 
    geom_density(aes(Y, color=W, fill=W), alpha=0.5) + ylab("") + 
    ggtitle("Distribution of Y in the treatment groups")
}
```

From these two variables, $W$ and $Y$, we can compute the following descriptive statistics:

- Average outcome:  $\mathbb{E}[Y]\approx$ `r round(mean(Y),2)`
- $Pr(Treatment=1)\approx$ `r mean(W)`
- $\mathbb{E}[(Y \,\mid\, Treatment=1)]\approx$ `r mean(Y*(W==1))/mean(W)`
- $\mathbb{E}[(Y \,\mid\, Treatment=0)]\approx$ `r mean(Y*(W==0))/mean(1-W)`


# Imputation

Before choosing an imputation method, it is important to explore the data and to
take a look at the missing values which might exhibit certain patterns. See the
appendix for an example of how to graphically examine the missing values.


We will impute the data with two different methods, iterative PCA and mice.

Note that we use both the outcome variable and the treatment assignment in the
imputation model as recommended by 
[[@seaman2014]](https://doi.org/10.1080/03610926.2012.700371).


## Via principal components

We now perform a **single** imputation with a (regularized) iterative PCA model, 
which will allow imputation to take into account 
similarities between both individuals and relationships between variables. 
For more details, see [[@le2008]](https://doi.org/10.18637/jss.v025.i01).



First we find the optimal number of dimensions for the imputation method by 
cross-validation.

**Note that if your data `X.na` contains only qualitative variables (i.e., categorical data), 
replace `estim_ncpFAMD` with `estim_ncpMCA`.**

```{r ncomp_all, eval=FALSE}
ncomp <- estim_ncpFAMD(data.frame(X.na, W = as.factor(W), Y = Y),
                        ncp.min = 1,
                        ncp.max=p,
                        method= "Regularized",
                        method.cv = "Kfold",
                        nbsim=10,
                        verbose = TRUE)

plot(1:length(ncomp$criterion), ncomp$criterion, xlab = "nb dim", ylab = "MSEP")
```


On this toy example, we do not run the above chunk to speed up the calculations.
We will arbitrarily set `ncp=3`.

**Same as for the `ncomp_all` chunk, replace `imputeFAMD` with `imputeMCA`.**
```{r pc_impute}
ncp <- 3
df.tmp <- data.frame(X.na, W = as.factor(W), Y = Y)
colnames(df.tmp) <- colnames(df.na)
df.imp.pc <- data.frame(imputeFAMD(df.tmp, ncp=ncp, seed=seed)$completeObs)
levels(df.imp.pc$W) <- c(0,1)
df.imp.pc$W <- as.numeric(as.character(df.imp.pc$W))
```


## MICE

We also do a **multiple** imputation analysis, using the `mice` package. We 
impute the data `m=5` times.

```{r mice_impute}
m <- 5
df.imp.mice <- mice(df.na, m=m, seed=seed, printFlag=F)
```


# ATE estimation

The  analyses that follow illustrate how to estimate the ATE. There exist 
however other similar estimands that differ slightly from the ATE but might be 
more relevant in certain applications You can produce estimates for these 
different estimands by changing the variable `target` in the chunk `
choose_target` below to

- `target="treated"` for the ATT
- `target ="control"` for the ATC
- `target=,"overlap"` for the ATE with overlap weights 

```{r choose_target}
target <- "all"
```


We estimate the ATE with two different estimators and two possibilities to estimate
nuisance parameters (i.e., the propensity scores and the conditional response surfaces).
A classical choice for nuisance parameter estimation in causal inference applications
is the use of generalized linear models (in practice, mostly linear and logistic models),
which we will use the `glm` function for.
And more recently, more complex and flexible models became available, such as
generalized regression forest [[@athey2019]](https://doi.org/10.1214/18-AOS1709).

```{r function_ate_on_df, echo=T}
treatment_effect_estimates <- function(data, target="all",
                                       confounder_names=NULL,
                                       only_treatment_pred_names=NULL,
                                       only_outcome_pred_names=NULL,
                                       seed=NULL) {
  if (is.null(confounder_names)) {
    confounder_names <- setdiff(colnames(data), c("W","Y"))
  }
  
  results <- list()
  results <- data.frame(matrix(ncol=4, nrow=0), row.names = NULL)
  
  if (sum(is.na(data))==0){
    tmp <- ipw(X=data[, confounder_names], 
               outcome=data$Y, treat=as.logical(data$W),
               ps.method="glm", target=target, 
               seed=seed)
    results <- rbind(results,
                     cbind("glm", "IPW_normalized_weights", tmp[[1]], tmp[[3]]))
    results <- rbind(results,
                     cbind("glm", "IPW_unnormalized_weights", tmp[[2]], tmp[[4]]))
  }
  
  tmp <- ipw(X=data[, confounder_names], 
             outcome=data$Y, treat=as.logical(data$W),
             ps.method="grf", target=target,
             seed=seed)
  results <- rbind(results,
                   cbind("grf", "IPW_normalized_weights", tmp[[1]], tmp[[3]]))
  results <- rbind(results,
                   cbind("grf", "IPW_unnormalized_weights", tmp[[2]], tmp[[4]]))

  if (sum(is.na(data))==0){
    tmp <- dr(X=data[, confounder_names], 
              X.for.ps = data[, c(only_treatment_pred_names,confounder_names)],
              X.for.outcome = data[, c(only_treatment_pred_names,confounder_names)],
              outcome = data$Y, treat = as.logical(data$W),
              ps.method = "glm.grf", out.method = "glm.grf",
              target = target, 
              seed=seed)
    results <- rbind(results,
                     cbind("glm", "DR", tmp[[1]], tmp[[2]]))
  }
  tmp <- dr(X=data[, confounder_names],
            X.for.ps = data[, c(only_treatment_pred_names,confounder_names)],
            X.for.outcome = data[, c(only_treatment_pred_names,confounder_names)],
            outcome = data$Y, treat = as.logical(data$W),
            ps.method = "grf.ate", out.method = "grf.ate",
            target = target,
            seed=seed)
  results <- rbind(results,
                   cbind("grf", "DR", tmp[[1]], tmp[[2]]))
  
  colnames(results) <- c("Nuisance.estimation",
                         "Estimate", 
                         "Value",
                         "StandardError")
  results$Value <- as.numeric(results$Value)
  results$StandardError <- as.numeric(results$StandardError)
  return(results)
}
```

```{r ate_on_pc_imp}
results_pc_imp <- treatment_effect_estimates(df.imp.pc, target=target, 
                                             seed=seed)
```

```{r ate_on_pc_mice, warning=F}
res_tmp <- complete(df.imp.mice, "all") %>% lapply(treatment_effect_estimates,target=target, seed=seed)
res_val <- as.data.frame(do.call(rbind, lapply(res_tmp, "[", , "Value")))
res_se <- as.data.frame(do.call(rbind, lapply(res_tmp, "[", , "StandardError")))
results_mice_imp <- cbind(res_tmp[[1]][,1:2], 
                          apply(res_val,2, mean), 
                          sapply(1:dim(res_tmp[[1]])[1], 
                                 function(j) sqrt(mean(res_se[,j]^2*n)+ (1+1/m)*sum((res_val[,j]-mean(res_val[,j]))^2)/(m-1))/sqrt(n)))
colnames(results_mice_imp) <- colnames(res_tmp[[1]]) 
```

```{r ate_on_xna}
results_mia <- treatment_effect_estimates(df.na, target=target,
                                             seed=seed)
```


# Results

## Table of all results
```{r table_results, echo=F, warning = F, message = F}
results <- data.frame(rbind(cbind("NA.handling"="pc.imp", results_pc_imp),
                 cbind("NA.handling"="mice", results_mice_imp), 
                 cbind("NA.handling"="mia", results_mia)), row.names = NULL)
knitr::kable(results, caption = "ATE estimations", digits = 2)
```

## Plots

We will plot all the previously calculated results.

```{r function_plot_combined, echo=F}
estimand <- switch(target,
                   "all" = "ATE", "treated" = "ATT", "control" = "ATC", "overlap" = "ATE_overlap",
                   stop("target must be either \"all\", \"treated\", \"control\", or \"overlap\""))

plot_treatment_effect <- function(res, estimate=c("IPW_normalized_weights", "DR"),
                                  nuisance.estimation=c("grf", "glm"), 
                                  na.handling=c("pc.imp","mice","mia")){ 
  df_plot <- res %>%
               filter(Nuisance.estimation %in% nuisance.estimation &
                      Estimate %in% estimate &
                      tolower(NA.handling) %in% na.handling)
  
  
  df_plot$CI.inf<- df_plot$Value - 1.96*df_plot$StandardError
  df_plot$CI.sup <- df_plot$Value + 1.96*df_plot$StandardError

  if (!("IPW_unnormalized_weights" %in% estimate)) {
    df_plot$Estimate[which(df_plot$Estimate=="IPW_normalized_weights")] <- "IPW"
    df_plot$Estimate <- as.factor(df_plot$Estimate)
    df_plot$Estimate <- droplevels(df_plot$Estimate)
  }
  
  df_plot$NA.Nuisance <- as.character(interaction(df_plot$NA.handling,
                                         df_plot$Nuisance.estimation))

  y.min <- min(c(ate_raw, df_plot$CI.inf), na.rm=T)
  y.min <- if_else(y.min<0, y.min*1.1, y.min*0.9)
  y.max <- max(c(ate_raw, df_plot$CI.sup), na.rm=T)
  y.max <- if_else(y.max<0, y.max*0.9, y.max*1.1)

  
  plt <- ggplot(data=df_plot[order(df_plot$NA.Nuisance),],
         aes(x=NA.Nuisance, y = Value, color=NA.handling)) +
         geom_errorbar(aes(ymin=CI.inf, ymax=CI.sup, linetype = Estimate), width=0.8, size=0.8)
  if (target == "all") {
    plt <- plt + geom_hline(aes(yintercept = ate_raw, colour = "ATE naive"), linetype = "dashed", size = 0.8)
  }
  if (synthetic & target == "all") {
    plt <- plt + geom_hline(aes(yintercept=tau, colour="ATE true"), show.legend = T)
  }
  plt <- plt + 
    coord_flip() +
    colScale +
    theme(axis.text=element_text(size=15),
          legend.position="bottom",
          legend.box="vertical", legend.margin=margin(),
          legend.text=element_text(size=14),
          legend.title=element_text(size=11)) +
    xlab("") +
    ylab(estimand) +
    ylim(c(y.min,y.max)) +
    labs(title=paste0(estimand, " estimation"), cex=0.7)
  
  return(plt)
}
```




```{r plot_ate, echo=F}
plt <- plot_treatment_effect(results)
print(plt)
```




# Appendix
## Missing values visualization

In general settings, we don't know how the missing values are generated. So 
let's look at the matrix plot to identify if there are any variables that tend 
to be missing together or if we can detect any clear violation of the missing 
completely at random hypothesis:

```{r matrixplot}
matrixplot(X.na, las=2, cex.axis = 0.3)
```


We can also run MCA on the missing and non missing entries to double check our 
conclusion and to check if other groups of variables tend to be missing together:
```{r na_mca}
data_miss <- data.frame(is.na(X.na))
data_miss <- apply(X=data_miss, FUN=function(x) if(x) "m" else "o", MARGIN=c(1,2))
res.mca <- MCA(data_miss, graph = FALSE)
plot(res.mca, invis = "ind", title = "MCA graph of the categories", cex =0.5)
```




## Balance plots

The standardized mean differences (SMD) higher than some threshold th, for 
instance th=0.1, indicate that the covariate distributions in the two groups 
differ: the treatment is not given uniformly at random. This explains the need 
for some adjustment or balancing in order to perform a causal analysis of the 
treatment on a certain outcome.

We use propensity score estimations from the `grf` package.


### On `X.imp.pc`
```{r balance_overlap_famd-grf, echo=F}
X.m = model.matrix(~. , data=data.frame(df.imp.pc[, confounder_names]))
forest.W = regression_forest(X.m, as.logical(W), tune.parameters = "all")
w.hat = predict(forest.W, X.m)$predictions
weights = as.numeric(as.logical(W))/w.hat + (1-as.numeric(as.logical(W)))/(1-w.hat)

balance <- bal.tab(df.imp.pc[, confounder_names], treat = W, 
                   estimand="ATE", continuous="std", weights = weights, method = "weighting", un=TRUE)

love.plot(x = balance, stat = "mean.diffs", abs = TRUE, var.order = "unadjusted", 
          threshold = 0.1, cex=0.8, 
          stars = "raw", shapes=c("circle", "triangle"),
          colors=c(viridis::viridis(10)[3], viridis::viridis(10)[9]))
```

We can also use the previously computed weights to look at the balance of the response pattern.
```{r balance_overlap_pc_mask-grf}
incomplete_confounders <- confounder_names[which(sapply(X.na[,confounder_names], function(x) sum(is.na(x))>0))]
R <- data.frame(is.na(X.na[,incomplete_confounders]))
colnames(R) <- paste(incomplete_confounders, "_NA", sep="")
balance <- bal.tab(R, treat = W, estimand="ATE", weights = weights, method = "weighting", un=TRUE)

love.plot(x = balance, stat = "mean.diffs", abs = TRUE, var.order = "unadjusted", 
          threshold = 0.1, cex=0.8, stars = "raw", shapes=c("circle", "triangle"),
          colors=c(viridis::viridis(10)[3], viridis::viridis(10)[9]))
```

```{r balance_overlap_famd_ps-grf}
bal.plot(x=data.frame(treat=as.logical(W), ps=w.hat, weights=weights), var.name = "ps", which = "both",
         treat=as.logical(W),
         weights=weights,
         type="histogram", mirror = TRUE)
```


### On `X.imp.mice`
```{r balance_overlap_mice-grf, echo=F}
w.hat <- list()
weights <- list()
df.ps <- list()
for (i in 1:m){
  df.ps[[i]] <- complete(df.imp.mice, i)[, confounder_names]
  X.m = model.matrix(~. , data=data.frame(df.ps[[i]]))
  forest.W = regression_forest(X.m, as.logical(complete(df.imp.mice, i)$W), tune.parameters = "all")
  w.hat[[i]] = predict(forest.W, X.m)$predictions
  weights[[i]] = as.numeric(as.logical(complete(df.imp.mice, i)$W))/w.hat[[i]] + (1-as.numeric(as.logical(complete(df.imp.mice, i)$W)))/(1-w.hat[[i]])
}
balance <- imputed_weighted <- list()
for (i in 1:m){
  balance[[i]] <- bal.tab(complete(df.imp.mice, i)[,confounder_names], 
                          treat = complete(df.imp.mice, i)$W,
                          estimand="ATE",
                          continuous="std", 
                          weights=weights[[i]],
                          method = "weighting",
                          un=TRUE)
  imputed_weighted[[i]] <- balance[[i]]$Balance[,c("Diff.Un", "Diff.Adj")]
}

for (i in 1:m){
  plt <- love.plot(x = balance[[i]], stat = "mean.diffs", abs = TRUE,
                   var.order = "unadjusted", threshold = 0.1, cex=0.8,
                   stars = "raw", shapes=c("circle", "triangle"),
                   colors=c(viridis::viridis(10)[3], viridis::viridis(10)[9]))
  print(plt)

  plt <- bal.plot(x=data.frame(treat=as.logical(complete(df.imp.mice, i)$W), ps=w.hat[[i]], weights=weights[[i]]), var.name = "ps", which = "both",
         treat=as.logical(complete(df.imp.mice, i)$W),
         weights=weights[[i]],
         type = "histogram", mirror = TRUE)
  print(plt)
}

# imputed_weighted_avg <- Reduce(`+`, imputed_weighted) / length(imputed_weighted)
# imputed_weighted_avg$Variable <-row.names(imputed_weighted_avg)
# imputed_weighted_avg <- imputed_weighted_avg %>%
#                           arrange(abs(Diff.Un))
# imputed_weighted_avg$Variable <- factor(imputed_weighted_avg$Variable, levels = imputed_weighted_avg$Variable)
# 
# #imputed_weighted_avg <- imputed_weighted_avg[-which(sapply(imputed_weighted_avg$Variable, function(x) grepl("<NA>", x, fixed=T))),]
# ggplot(data=imputed_weighted_avg) + 
#   geom_point(aes(x = Variable, y = abs(Diff.Un), shape="unadj", color = "unadj"), size=2.5) +
#   geom_point(aes(x = Variable, y = abs(Diff.Adj), shape="adj", color = "adj"), size=2.5) +
#   scale_shape_manual(name="Sample",
#                      values = c(adj="triangle", unadj="circle"),
#                      labels = rev(c("Adjusted","Unadjusted")))+
#   scale_color_manual(name="",
#                      values = c(adj=viridis::viridis(10)[9], unadj=viridis::viridis(10)[3]),
#                      labels = rev(c("","")))+
#   geom_hline(yintercept = 0.1, linetype = "dashed", size=0.4) +
#   geom_hline(yintercept = 0, size=0.4) +
#   ylab("Absolute Mean Differences")+
#   xlab("")+
#   labs(title="Covariate Balance") +
#   coord_flip() + 
#   theme_minimal() + 
#   theme(panel.grid.major = element_blank(), 
#         panel.grid.minor = element_blank(),
#         panel.background = element_rect(colour = "black", size=0.4),
#         axis.text.y = element_text(color="black", size=9),
#         axis.text.x = element_text(color="black"),
#         axis.ticks = element_line(colour = "black", size = 0.2),
#         axis.line = element_line(colour = 'transparent', size = 0.4),
#         plot.title = element_text(size=13, hjust=0.5))
```

### On `X.na` using MIA
```{r balance_overlap_grf-mia, echo=FALSE}
na.action <- options()$na.action
options(na.action='na.pass')
X.m = model.matrix(~. , data=X.na[,confounder_names])
options(na.action=na.action)
forest.W = regression_forest(X.m, as.logical(W), tune.parameters = "all")
w.hat = predict(forest.W, X.m)$predictions
weights = as.numeric(as.logical(W))/w.hat + (1-as.numeric(as.logical(W)))/(1-w.hat)

bal.plot(x=data.frame(treat=as.logical(W), ps=w.hat, weights=weights), 
         var.name = "ps", which = "both",
         treat=as.logical(W),
         weights=weights,
         type = "histogram", mirror = TRUE)

balance <- bal.tab(X.na[, confounder_names], treat = W, estimand="ATE", 
                   continuous="std", weights = weights, method = "weighting", 
                   un=TRUE)
love.plot(x = balance, stat = "mean.diffs", abs = TRUE, var.order = "unadjusted", 
          stars="raw",continuous="std", threshold = 0.1, cex=0.8, 
          shapes=c("circle", "triangle"), 
          colors=c(viridis::viridis(10)[3], viridis::viridis(10)[9]))

balance <- bal.tab(R, treat = W, estimand="ATE", weights = weights, un=TRUE)

love.plot(x = balance, stat = "mean.diffs", abs = TRUE, var.order = "unadjusted", 
          threshold = 0.1, cex=0.8, , stars = "raw", shapes=c("circle", "triangle"),
          colors=c(viridis::viridis(10)[3], viridis::viridis(10)[9]))
```

# References